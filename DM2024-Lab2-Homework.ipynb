{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Ric Chen\n",
    "\n",
    "Student ID: 110506025\n",
    "\n",
    "GitHub ID: uilfl\n",
    "\n",
    "Kaggle name:ricdatadog\n",
    "\n",
    "Kaggle_notebook : https://www.kaggle.com/code/ricdatadog/newmodel\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "![snapshot](DM2024-Lab2-Homework/Screenshot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 8th, 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report: Model Development for Text Classification Competition\n",
    "\n",
    "Introduction\n",
    "\n",
    "This report outlines the development of a text classification model for the competition, incorporating various preprocessing techniques, feature engineering steps, and modeling approaches. The primary goal was to analyze and classify tweets based on their emotional tone or content. The report discusses the methodologies used, challenges encountered, and insights gained during the process.\n",
    "\n",
    "Data Preprocessing\n",
    "\n",
    "Effective data preprocessing is a cornerstone of any text-based machine learning project. Several steps were implemented to ensure the dataset was clean and ready for modeling.\n",
    "\n",
    "Steps Taken:\n",
    "\t1.\tHandling Missing Values:\n",
    "\t•\tRemoved rows where critical columns like text or emotion had missing values.\n",
    "\t2.\tTokenization:\n",
    "\t•\tTweets were tokenized into individual words using nltk.\n",
    "\t3.\tStopword Removal:\n",
    "\t•\tStandard stopwords were removed to reduce noise.\n",
    "\t•\tIdentified the need to expand the stopwords list with domain-specific terms in future iterations.\n",
    "\t4.\tSpecial Character Cleaning:\n",
    "\t•\tRemoved Twitter-specific symbols (@, #, URLs) and other non-alphanumeric characters.\n",
    "\t•\tRetained only the textual content necessary for emotion classification.\n",
    "\t5.\tLabel Encoding:\n",
    "\t•\tEncoded emotion labels into integers using LabelEncoder.\n",
    "\t6.\tWord Embeddings:\n",
    "\t•\tUsed pretrained Word2Vec embeddings to represent tokens as dense vectors. Sentence-level embeddings were computed as the mean of word vectors.\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "To enhance the dataset’s representation, multiple feature engineering approaches were employed:\n",
    "\t1.\tTF-IDF Vectorization:\n",
    "\t•\tInitially used TF-IDF to transform text into sparse numerical vectors.\n",
    "\t•\tProvided a strong baseline for simple models but lacked the contextual understanding required for deeper insights.\n",
    "\t2.\tPretrained Embeddings:\n",
    "\t•\tIntegrated Word2Vec to generate dense, context-aware vector representations of text.\n",
    "\t•\tThese embeddings formed the basis for clustering and classification models.\n",
    "\t3.\tClustering for Insights:\n",
    "\t•\tApplied DBSCAN to cluster tweets based on their embeddings.\n",
    "\t•\tVisualized clusters using PCA to reduce dimensionality and identify patterns.\n",
    "\n",
    "Model Development\n",
    "\n",
    "Deep Learning Approach\n",
    "\t•\tDeveloped a deep learning model to classify text based on TF-IDF features.\n",
    "\t•\tKey characteristics:\n",
    "\t•\tIntegrated an embedding layer to process text features.\n",
    "\t•\tTrained the model with multiple epochs, observing convergence and overfitting trends.\n",
    "\t•\tResults:\n",
    "\t•\tAchieved approximately 50% accuracy, indicating the need for architectural refinement and enhanced feature representation.\n",
    "\n",
    "Classical Machine Learning Models\n",
    "\n",
    "To complement the deep learning approach, traditional models were also explored:\n",
    "\t1.\tRandom Forest Classifier:\n",
    "\t•\tTrained on Word2Vec embeddings.\n",
    "\t•\tDelivered competitive accuracy with an interpretable decision-making process.\n",
    "\t2.\tNaive Bayes Classifier:\n",
    "\t•\tApplied to TF-IDF features.\n",
    "\t•\tPerformed well for initial text representation but struggled with nuanced classes.\n",
    "\n",
    "Insights and Challenges\n",
    "\n",
    "Insights:\n",
    "\t•\tWord embeddings provided a richer feature representation compared to TF-IDF.\n",
    "\t•\tClustering revealed underlying patterns and the presence of noise in the dataset.\n",
    "\t•\tDifferent models highlighted the trade-off between complexity and interpretability.\n",
    "\n",
    "Challenges:\n",
    "\t•\tHandling imbalanced classes in the dataset.\n",
    "\t•\tAchieving alignment between model outputs and competition requirements (e.g., ensuring 411,972 rows in predictions).\n",
    "\t•\tFine-tuning hyperparameters for optimal performance across diverse models.\n",
    "\n",
    "Recommendations for Future Work\n",
    "\t1.\tEnhanced Preprocessing:\n",
    "\t•\tDevelop a more comprehensive cleaning pipeline tailored to Twitter data.\n",
    "\t•\tExpand the stopwords list with competition-specific terms.\n",
    "\t2.\tExperiment with Pretrained Models:\n",
    "\t•\tLeverage transformer-based embeddings (e.g., BERT, RoBERTa) for deeper contextual understanding.\n",
    "\t3.\tFine-Tune Deep Learning Models:\n",
    "\t•\tExperiment with architectures like LSTM, GRU, and transformers.\n",
    "\t•\tUse learning rate schedulers and regularization techniques to prevent overfitting.\n",
    "\t4.\tAugment Data for Class Imbalance:\n",
    "\t•\tApply oversampling techniques like SMOTE.\n",
    "\t•\tGenerate synthetic text samples using NLP-based augmentation tools.\n",
    "\t5.\tHybrid Model Approach:\n",
    "\t•\tCombine predictions from classical and deep learning models to balance interpretability and performance.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The project successfully demonstrated the integration of multiple preprocessing, feature engineering, and modeling techniques. While the current models provided a solid foundation, future iterations focusing on advanced embeddings, better preprocessing, and enhanced architectures will likely yield significant improvements. This iterative process showcases the potential of machine learning in text classification tasks and provides valuable lessons for similar challenges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = emotion_tweets.dropna(subset=['text', 'emotion'])\\ndf['tokens'] = df['text'].apply(word_tokenize)\\ndf['emotion'] = LabelEncoder().fit_transform(df['emotion'])\\n\\n# Generate Word2Vec embeddings\\nw2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\\ndf['embedding'] = df['tokens'].apply(lambda x: get_sentence_embedding(x, w2v_model))\\n\\n# Train-test split\\nX = np.array(df['embedding'].tolist())\\ny = df['emotion']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Random Forest\\nrf_model = RandomForestClassifier(random_state=42)\\nrf_model.fit(X_train, y_train)\\nrf_preds = rf_model.predict(X_test)\\n\\n# Evaluate and visualize\\nprint(classification_report(y_test, rf_preds))\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Summary\n",
    "#Below is a summary of the code used to preprocess data, engineer features, and train models:\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "\n",
    "'''\n",
    "df = emotion_tweets.dropna(subset=['text', 'emotion'])\n",
    "df['tokens'] = df['text'].apply(word_tokenize)\n",
    "df['emotion'] = LabelEncoder().fit_transform(df['emotion'])\n",
    "\n",
    "# Generate Word2Vec embeddings\n",
    "w2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "df['embedding'] = df['tokens'].apply(lambda x: get_sentence_embedding(x, w2v_model))\n",
    "\n",
    "# Train-test split\n",
    "X = np.array(df['embedding'].tolist())\n",
    "y = df['emotion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate and visualize\n",
    "print(classification_report(y_test, rf_preds))\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
